{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data & Things (RUC F2023)\n",
    "\n",
    "## Hand-in Exercises for Exam\n",
    "\n",
    "* This is a template for your exercise solutions. Each solution may use multiple cells. \n",
    "\n",
    "* Do your best to make your code clean and clear, e.g., by using comments and markdowns.\n",
    "\n",
    "* Remeber to fill in the information of all your group members in the following cell."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Members:\n",
    "* [Rasmus Kjær Nielsen, 68910, rkjaern@ruc.dk]\n",
    "* [name_2, student number, email_2]\n",
    "* [Add more if needed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Loading of common modules or initialization of other common things, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scratch.deep_learning as dl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "import mglearn\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.cluster import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. EDA and data cleaning (Lecture 2 & 5)\n",
    "\n",
    "Make an Exploratory Data Analysis (EDA) and data cleaning of the “titanic_survival_data.csv” dataset from Lectures 5 and 6, including dealing with outliers and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[\"Cabin\"].fillna(\"Unknown\", inplace=True)\n",
    "titanic.dropna(subset=[\"Age\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[\"Sex\"] = titanic.Sex.replace({'male':0, 'female':1})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all possible categories for the \"PClass\" column\n",
    "print(f\"Possible values for PClass: {titanic['Pclass'].unique()}\")\n",
    "\n",
    "# Use Pandas to One-Hot encode the PClass category\n",
    "dataset_with_one_hot = pd.get_dummies(titanic, columns=[\"Pclass\"], drop_first=False)\n",
    "\n",
    "# Add back in the old Pclass column, for learning purposes\n",
    "dataset_with_one_hot[\"Pclass\"] = titanic.Pclass\n",
    "\n",
    "# Print out the first few rows\n",
    "dataset_with_one_hot.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same principle. We convert from cabin number and narrow it down to corresponding deck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_one_hot = pd.get_dummies(titanic, columns=[\"Pclass\", \"Cabin\"], drop_first=False)\n",
    "\n",
    "cabin_column_names = list(c for c in dataset_with_one_hot.columns if c.startswith(\"Cabin_\"))\n",
    "\n",
    "print(len(cabin_column_names), \"cabins found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[\"Deck\"] = [c[0] for c in titanic.Cabin]\n",
    "\n",
    "print(\"Decks: \", sorted(titanic.Deck.unique()))\n",
    "\n",
    "dataset_with_one_hot = pd.get_dummies(titanic, columns=[\"Pclass\", \"Deck\"], drop_first=False)\n",
    "\n",
    "deck_of_cabin_column_names = list(c for c in dataset_with_one_hot.columns if c.startswith(\"Deck_\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification (Lecture 3 & 4)\n",
    "\n",
    "Combine the exercise from Lecture 3 with exercise 2 from Lecture 4 into one, and construct some classification models to predict if a passenger would survive or not in the Titanic dataset. \n",
    "\n",
    "* a) You should have (1) decision tree, (2) random forest, and (3) KNN. You may also vary the configuration of each model type.\n",
    "* b) You should do necessary data preprocessing (e.g., missing value fill-in, and data scaling if needed for a classifier). \n",
    "* c) You should also do cross-validation of your models.\n",
    "* d) Plot the ROC with AUC for each model you implement.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises with KNN on Titanic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have handled the data, and filled in the missing values. So now we can do the KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = titanic.columns.drop(['Survived', 'Name', 'Ticket', 'Cabin', 'Deck', 'Embarked', 'PassengerId'])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic[features]\n",
    "y = titanic['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#criterion='entropy', max_features=5, n_estimators=100, random_state=0)\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=0) \n",
    "forest.fit(X_train, y_train)\n",
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "print(\"Accuracy of Random Forest: {}\".format(metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_feature_importances(model, features):\n",
    "    n_features = len(features)\n",
    "    plt.barh(np.arange(n_features), model.feature_importances_, align='center')\n",
    "    \n",
    "    plt.yticks(np.arange(n_features), features)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim(-1, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(forest, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the features with most importance towards predicting survival is Fare, Sex and Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic[['Age', 'Fare', 'Sex']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "def knnRunner(X, y):\n",
    "    # We create a 2-D array to store all accuracy values\n",
    "    accuracy_data1 = []\n",
    "    \n",
    "    for training_percent in [0.7, 0.8, 0.9]:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-training_percent, random_state=1)\n",
    "\n",
    "        # We crate a 1-D array to store all accuracy values on this split batch\n",
    "        accuracy_row = []\n",
    "        \n",
    "        for k in range(2, 11):\n",
    "            # Model intializing\n",
    "            knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "            # Training (very light compared to an eager learner)\n",
    "            knn.fit(X_train, y_train)\n",
    "\n",
    "            # Validation/Test\n",
    "            y_pred = knn.predict(X_test)\n",
    "\n",
    "            print(\"KNN with training_percent={}, k={}:\\r\".format(training_percent, k))\n",
    "            \n",
    "            # Get the accuracy from metrics\n",
    "            accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "            print(\"Accuracy: {}\\r\\n\".format(accuracy))\n",
    "\n",
    "            accuracy_row.append(accuracy)\n",
    "            \n",
    "        accuracy_data1.append(accuracy_row)\n",
    "    \n",
    "    return accuracy_data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the data without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_no_scaling = knnRunner(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with standardscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "X_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_standard_scaling = knnRunner(X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with min-max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minMaxScaler = MinMaxScaler()\n",
    "X_scaled_mm = pd.DataFrame(minMaxScaler.fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_minmax_scaling = knnRunner(X_scaled_mm, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualized in a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "index = ['0.7', '0.8', '0.9']\n",
    "\n",
    "df_no_scaling = pd.DataFrame(accuracies_no_scaling, columns=columns, index=index).transpose()\n",
    "df_standard_scaling = pd.DataFrame(accuracies_standard_scaling, columns=columns, index=index).transpose()\n",
    "df_minmax_scaling = pd.DataFrame(accuracies_minmax_scaling, columns=columns, index=index).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAccuracy(col):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.plot(df_standard_scaling.index, df_standard_scaling[col], 'b.-', label = 'standard scaling', color='pink')\n",
    "    plt.plot(df_minmax_scaling.index, df_minmax_scaling[col], 'b.-', label = 'minmax scaling', color='blue')\n",
    "    plt.plot(df_no_scaling.index, df_no_scaling[col], 'b.-', label = 'no scaling', color='green')\n",
    "\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('KNN accuracy')\n",
    "    plt.title('Effect of data scaling on KNN for training data ratio={}'.format(col))\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAccuracy('0.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAccuracy('0.8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAccuracy('0.9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "index = ['0.7', '0.8', '0.9']\n",
    "\n",
    "df_standard_scaling = pd.DataFrame(accuracies_standard_scaling, columns=columns, index=index).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(df_standard_scaling.index, df_standard_scaling['0.9'], 'b.-', label = '0.9', color='green')\n",
    "plt.plot(df_standard_scaling.index, df_standard_scaling['0.8'], 'b.-', label = '0.8', color='blue')\n",
    "plt.plot(df_standard_scaling.index, df_standard_scaling['0.7'], 'b.-', label = '0.7', color='pink')\n",
    "\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('KNN accuracy')\n",
    "plt.title('Effect of training data percentage on KNN')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression (Lecture 6)\n",
    "\n",
    "Train a multiple linear regression, a random forest model, and an AdaBoost model on the “boston_housing_data.csv” dataset from Lectures 5 and 6 and remember to do train-test split as well as other necessary pre-processing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering (Lecture 7 & 8)\n",
    "\n",
    "Exercise 2 (both 2.1 and 2.2) from Lecture 7 and exercise 1 from Lecture 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/Ch5_bike_station_locations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to show the data head and tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we can plot this, to show that there is clusters showing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['latitude']\n",
    "y = data['longitude']\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = StandardScaler()\n",
    "data[['latitude','longitude']] = z.fit_transform(data[['latitude','longitude']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cant necessarily tell how many clusters we can create from this. But to help us, we can use DBScan and different methods to determine the amount of clusters we need for a conclusive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5,min_samples=5)\n",
    "clusters = dbscan.fit_predict(data)\n",
    "print(\"Cluster memberships:\\n{}\".format(clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to have values of -1, then we would define that as an outlier. What this basically mean is that, when we generalize all points to be within epsilon distance of at least 5 points, then with our specific epsilon, we can encapsulate all points with at least 5 points within epsilon distance. However, if we were to change our epsilon to something signifanctly smaller, we will have most definitely see some outliers, or some -1 from our dbscan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_dbscan()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, we can see the amout of clusters that we create with different epsilons and min_samples. We notice that we get 3 pretty solid divided clusters with epsilon 1.5 and the minimum samples being 2 and 3. The others either simplify into 2 clusters or one big cluster. But we can further see if these are the appropriate amount of clusters, by using the Silhouette method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(seed=0)\n",
    "random_clusters = random_state.randint(low=0, high=2, size=len(data))\n",
    "def compare_clustering_algs(score_func, ref):\n",
    "    \"\"\"\n",
    "    For all included clustering algorithms, get the socre by using \n",
    "    the score function *score_func* against the reference *ref*\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a list of algorithms to use\n",
    "    algorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2), DBSCAN()]\n",
    "\n",
    "    # Set four sub figures for four approaches\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 3), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "\n",
    "    # First, we plot the random cluster assignment\n",
    "    axes[0].scatter(data['latitude'], data['longitude'], c=random_clusters, cmap=mglearn.cm3, s=60)\n",
    "    # The score function compares the groundtruth cluter ids in y and the clustering result\n",
    "    axes[0].set_title(\"Random assignment: score={:.2f}\".format(score_func(ref, random_clusters)))\n",
    "\n",
    "    # Next, we plot the three clustering algorithms' result\n",
    "    for ax, algorithm in zip(axes[1:], algorithms):\n",
    "        # Plot the current algorithm's cluster assignment and cluster centers\n",
    "        clusters = algorithm.fit_predict(data)\n",
    "        ax.scatter(data['latitude'], data['longitude'], c=clusters, cmap=mglearn.cm3, s=60)\n",
    "        ax.set_title(\"{}: score={:.2f}\".format(algorithm.__class__.__name__, score_func(ref, clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_clustering_algs(silhouette_score, data)\n",
    "# fig = plt.gcf()\n",
    "# fig.suptitle(\"Silhouette Score\", 0.5, 1.05, fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key-value stores (Lecture 9)\n",
    "\n",
    "Exercise 1 from Lecture 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep learning Lecture 10\n",
    "\n",
    "Train a deep neural network to predict if a passenger would survive or not in the Titanic dataset and remember to do train-test split as well as other necessary pre-processing dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to use random forrest evaluation find the features that have the most influence towards survivability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = titanic.columns.drop(['Survived', 'Name', 'Ticket', 'Cabin', 'Deck', 'Embarked', 'PassengerId'])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic[features]\n",
    "y = titanic['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#criterion='entropy', max_features=5, n_estimators=100, random_state=0)\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=0) \n",
    "forest.fit(X_train, y_train)\n",
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "print(\"Accuracy of Random Forest: {}\".format(metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_feature_importances(model, features):\n",
    "    n_features = len(features)\n",
    "    plt.barh(np.arange(n_features), model.feature_importances_, align='center')\n",
    "    \n",
    "    plt.yticks(np.arange(n_features), features)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim(-1, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(forest, features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the features with most importance towards predicting survival is Fare, Sex and Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic[['Age', 'Fare', 'Sex']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values.tolist()\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_oh = [dl.one_hot_encode(y, 2) for y in y_train]\n",
    "y_train_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_oh = [dl.one_hot_encode(y, 2) for y in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "    \n",
    "# Name them so we can turn train on and off\n",
    "dropout1 = dl.Dropout(0.1)\n",
    "dropout2 = dl.Dropout(0.1)\n",
    "dropout3 = dl.Dropout(0.1)\n",
    "    \n",
    "t_model = dl.Sequential([\n",
    "    dl.Linear(3, 32),  # Hidden layer 1: size 32\n",
    "    dropout1,\n",
    "    dl.Tanh(),\n",
    "    dl.Linear(32, 16),   # Hidden layer 2: size 16\n",
    "    dropout2,\n",
    "    dl.Tanh(),\n",
    "    dl.Linear(16, 8),   # Hidden layer 3: size 8\n",
    "    dropout3,\n",
    "    dl.Tanh(),\n",
    "    dl.Linear(8, 2)    # Output layer: size 2\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "import tqdm\n",
    "def loop(model: dl.Layer,\n",
    "             images: dl.List[dl.Tensor],\n",
    "             labels: dl.List[dl.Tensor],\n",
    "             loss: dl.Loss,\n",
    "             optimizer: dl.Optimizer = None) -> None:\n",
    "        correct = 0         # Track number of correct predictions.\n",
    "        total_loss = 0.0    # Track total loss.\n",
    "    \n",
    "        with tqdm.trange(len(images)) as t:\n",
    "            for i in t:\n",
    "                predicted = model.forward(images[i])             # Predict.\n",
    "                if argmax(predicted) == argmax(labels[i]):       # Check for\n",
    "                    correct += 1                                 # correctness.\n",
    "                total_loss += loss.loss(predicted, labels[i])    # Compute loss.\n",
    "    \n",
    "                # If we're training, backpropagate gradient and update weights.\n",
    "                if optimizer is not None:\n",
    "                    gradient = loss.gradient(predicted, labels[i])\n",
    "                    model.backward(gradient)\n",
    "                    optimizer.step(model)\n",
    "    \n",
    "                # And update our metrics in the progress bar.\n",
    "                avg_loss = total_loss / (i + 1)\n",
    "                acc = correct / (i + 1)\n",
    "                t.set_description(f\"mnist loss: {avg_loss:.3f} acc: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.deep_learning import main\n",
    "optimizer = dl.Momentum(learning_rate=0.01, momentum=0.99)\n",
    "loss = dl.SoftmaxCrossEntropy()\n",
    "    \n",
    "# Enable dropout and train (takes > 20 minutes on my laptop!)\n",
    "dropout1.train = dropout2.train = dropout3.train = True\n",
    "loop(t_model, X_train, y_train_oh, loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable dropout and evaluate\n",
    "dropout1.train = dropout2.train = dropout3.train = False\n",
    "loop(t_model, X_test, y_test_oh, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [t_model.forward(x) for x in X_test]\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.softmax(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary = [1 if x[1] > 0.5 else 0 for x in dl.softmax(y_pred)]\n",
    "y_pred_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_binary))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, y_pred_binary))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred_binary))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = dl.Momentum(learning_rate=0.001, momentum=0.99)\n",
    "\n",
    "dropout1.train = dropout2.train = dropout3.train = True\n",
    "\n",
    "for _ in range(15):\n",
    "    loop(t_model, X_train, y_train_oh, loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout1.train = dropout2.train = False\n",
    "loop(t_model, X_test, y_test_oh, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [t_model.forward(x) for x in X_test]\n",
    "y_pred_binary = [1 if x[1] > 0.5 else 0 for x in dl.softmax(y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_binary))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, y_pred_binary))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred_binary))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred_binary))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(3)),  # As we have 3 columns in our input data X\n",
    "        layers.Dense(32, activation='tanh'),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(16, activation='tanh'),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(8, activation='tanh'),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(2, activation=\"softmax\")  # Here we specify that we want the last layer to have a softmax activation function\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_keras = np.array(X_train)\n",
    "X_train_keras.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_keras = np.array(y_train_oh)\n",
    "y_train_keras.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.fit(X_train_keras, y_train_keras, batch_size=1, epochs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = keras_model.evaluate(np.array(X_test), np.array(y_test_oh), verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_keras = keras_model.predict(np.array(X_test))\n",
    "y_pred_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_keras_binary = [1 if x[1] > 0.5 else 0 for x in y_pred_keras]\n",
    "y_pred_keras_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_keras_binary))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, y_pred_keras_binary))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred_keras_binary))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred_keras_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. MapReduce (Lecture 13)\n",
    "\n",
    "All exercises from Lecture 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Time Series Analysis (Lecture 14 & 15)\n",
    "\n",
    "Do a time series analysis of the Copenhagen ice cream dataset (\"cph_ice_cream_searches.csv\") from Lectures 14 and 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. IoT (Lecture 17)\n",
    "\n",
    "All exercises from Lecture 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have the data. Next we do is choose our features. We do this by dropping the other columns that we dont want. We also drop the Outcome feature, since it's our class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.columns.drop(['Pregnancies','SkinThickness','BMI','DiabetesPedigreeFunction','Age','Outcome'])\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our x and y for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[features]\n",
    "y = data['Outcome']\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next exercise, is to Normalize our features to between [0,1] using MinMax scaling. The MinMax datascaling works by subtracting the minimum value in the feature and then divides by the range. Where the range is the difference between the original maximum and original minimum. So if the max is 5 and the minimum is 1, and we have the value 3. Then we say (3-1)/4=0.5. What this does, is ensuring that all of our data is scaled between 0 and 1, which is an easier way to represent outlier data and show it compared to the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minMaxScaler = MinMaxScaler(feature_range=(0,1))\n",
    "X_scaled = pd.DataFrame(minMaxScaler.fit_transform(x), columns = x.columns)\n",
    "X_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now normalized our data. The next exercise involves saving the scaled data into another dataset. So firstly we change y to being a dataframe, and then we concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yf = pd.DataFrame(y, columns=['Outcome'])\n",
    "S = pd.concat([X_scaled,y], axis=1)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = S.columns.drop('Outcome')\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = S[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= X[:100]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = S[['Outcome']]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y[:100]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next exercise is regarding MQTT (Message Queue Telemetry Transport). It's a standard messaging protocol for IoT. Before sending or recieving a message, a client must first connect to a server. So we make both a broker and a client. The description of the exercise is: \"The subscriber continuously receives the data. For each latest record r received, apply the \n",
    "3NN classification to the last 5 records before r, and compare the classification result with the \n",
    "Outcome label in r\". But first, I will make a training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values.tolist()\n",
    "X_test = X_test.values.tolist()\n",
    "y_train = y_train.values.tolist()\n",
    "y_test = y_test.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our training, then we can set up our broker and subscriber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import paho.mqtt.client as mqtt\n",
    "# We create a client as the data subscriber and specify its actions for particular events\n",
    "mqttc = mqtt.Client()\n",
    "\n",
    "# Now, we connect to the data broker.\n",
    "mqttc.connect(\"mqtt.eclipseprojects.io\", 1883, 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Publishing...\")\n",
    "    \n",
    "    index = 0\n",
    "    while (index >= 0):\n",
    "        # Get the current data reading to send out\n",
    "        data = X_train[index][0]\n",
    "        print(data)\n",
    "       \n",
    "        # Publish the data reading as 'Diabetes/Glucose'\n",
    "        mqttc.publish(\"Glucose\", str(data))\n",
    "        # We send the next reading after 2 seconds\n",
    "        time.sleep(0.2)\n",
    "        if index == len(X_train) -1:\n",
    "            index = -1\n",
    "        else:\n",
    "            index = index + 1\n",
    "\n",
    "# This function defines what to do when we connect to the broker\n",
    "def on_connect(client, userdata, flags, rc):\n",
    "    print(\"Connected with result code \" + str(rc))\n",
    "    # We subscribe to this particular data. There may be other data published by the same subscriber or broker.\n",
    "    client.subscribe(\"Glucose\")\n",
    "\n",
    "# This function defines what to do when we receives a message from the brokder\n",
    "def on_message(client, userdata, msg):\n",
    "    print(\"Received Glucose: \", msg.payload.decode())\n",
    "    global current_glucose\n",
    "    current_glucose = msg.payload.decode()\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    print(f'prediction {y_pred}: real {y_test}')\n",
    "\n",
    "# We create a client as the data subscriber and specify its actions for particular events\n",
    "mqttc = mqtt.Client()\n",
    "mqttc.on_connect = on_connect\n",
    "mqttc.on_message = on_message\n",
    "\n",
    "# Now, we connect to the data broker.\n",
    "mqttc.connect(\"mqtt.eclipseprojects.io\", 1883, 60)\n",
    "# As a simple example, we just keep the data listening/receiving on and on...\n",
    "mqttc.loop_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import html, dcc, Input, Output\n",
    "\n",
    "import dash_bootstrap_components as dbc\n",
    "import paho.mqtt.client as mqtt\n",
    "\n",
    "global current_glucose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Defining Dash app\n",
    "# -----------------------------------------------------------------------------\n",
    "app = dash.Dash(external_stylesheets=[dbc.themes.DARKLY])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Glucose card\n",
    "# -----------------------------------------------------------------------------\n",
    "card = dbc.Card(\n",
    "    html.H4(id=\"Glucose\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Application layout\n",
    "# -----------------------------------------------------------------------------\n",
    "app.layout = dbc.Container(\n",
    "    [\n",
    "        dcc.Interval(id='update', n_intervals=0, interval=1000*3),\n",
    "        html.H1(\"Monitoring IoT Sensor Data with Plotly Dash\"),\n",
    "        html.Hr(),\n",
    "        dbc.Row(dbc.Col(card, lg=4))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Callback for updating Glucose data\n",
    "# -----------------------------------------------------------------------------\n",
    "@app.callback(\n",
    "    Output('Glucose', 'children'),\n",
    "    Input('update', 'n_intervals')\n",
    ")\n",
    "\n",
    "def update_glucose(timer):\n",
    "    return (\"Glucose: \" + str(current_glucose) + \" at time \" + str(timer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Main function\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    app.run_server(debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
