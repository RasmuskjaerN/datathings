{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data & Things (RUC F2023)\n",
    "\n",
    "## Hand-in Exercises for Exam\n",
    "\n",
    "* This is a template for your exercise solutions. Each solution may use multiple cells. \n",
    "\n",
    "* Do your best to make your code clean and clear, e.g., by using comments and markdowns.\n",
    "\n",
    "* Remeber to fill in the information of all your group members in the following cell."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Members:\n",
    "* [Rasmus Kjær Nielsen, 68910, rkjaern@ruc.dk]\n",
    "* [name_2, student number, email_2]\n",
    "* [Add more if needed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Loading of common modules or initialization of other common things, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scratch.deep_learning as dl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. EDA and data cleaning (Lecture 2 & 5)\n",
    "\n",
    "Make an Exploratory Data Analysis (EDA) and data cleaning of the “titanic_survival_data.csv” dataset from Lectures 5 and 6, including dealing with outliers and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[\"Cabin\"].fillna(\"Unknown\", inplace=True)\n",
    "titanic.dropna(subset=[\"Age\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[\"Sex\"] = titanic.Sex.replace({'male':0, 'female':1})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all possible categories for the \"PClass\" column\n",
    "print(f\"Possible values for PClass: {titanic['Pclass'].unique()}\")\n",
    "\n",
    "# Use Pandas to One-Hot encode the PClass category\n",
    "dataset_with_one_hot = pd.get_dummies(titanic, columns=[\"Pclass\"], drop_first=False)\n",
    "\n",
    "# Add back in the old Pclass column, for learning purposes\n",
    "dataset_with_one_hot[\"Pclass\"] = titanic.Pclass\n",
    "\n",
    "# Print out the first few rows\n",
    "dataset_with_one_hot.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same principle. We convert from cabin number and narrow it down to corresponding deck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_one_hot = pd.get_dummies(titanic, columns=[\"Pclass\", \"Cabin\"], drop_first=False)\n",
    "\n",
    "cabin_column_names = list(c for c in dataset_with_one_hot.columns if c.startswith(\"Cabin_\"))\n",
    "\n",
    "print(len(cabin_column_names), \"cabins found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[\"Deck\"] = [c[0] for c in titanic.Cabin]\n",
    "\n",
    "print(\"Decks: \", sorted(titanic.Deck.unique()))\n",
    "\n",
    "dataset_with_one_hot = pd.get_dummies(titanic, columns=[\"Pclass\", \"Deck\"], drop_first=False)\n",
    "\n",
    "deck_of_cabin_column_names = list(c for c in dataset_with_one_hot.columns if c.startswith(\"Deck_\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification (Lecture 3 & 4)\n",
    "\n",
    "Combine the exercise from Lecture 3 with exercise 2 from Lecture 4 into one, and construct some classification models to predict if a passenger would survive or not in the Titanic dataset. \n",
    "\n",
    "* a) You should have (1) decision tree, (2) random forest, and (3) KNN. You may also vary the configuration of each model type.\n",
    "* b) You should do necessary data preprocessing (e.g., missing value fill-in, and data scaling if needed for a classifier). \n",
    "* c) You should also do cross-validation of your models.\n",
    "* d) Plot the ROC with AUC for each model you implement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression (Lecture 6)\n",
    "\n",
    "Train a multiple linear regression, a random forest model, and an AdaBoost model on the “boston_housing_data.csv” dataset from Lectures 5 and 6 and remember to do train-test split as well as other necessary pre-processing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering (Lecture 7 & 8)\n",
    "\n",
    "Exercise 2 (both 2.1 and 2.2) from Lecture 7 and exercise 1 from Lecture 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key-value stores (Lecture 9)\n",
    "\n",
    "Exercise 1 from Lecture 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep learning (Lecture 10)\n",
    "\n",
    "Train a deep neural network to predict if a passenger would survive or not in the Titanic dataset and remember to do train-test split as well as other necessary pre-processing dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to use random forrest evaluation find the features that have the most influence towards survivability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = titanic.columns.drop(['Survived', 'Name', 'Ticket', 'Cabin', 'Deck', 'Embarked', 'PassengerId'])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic[features]\n",
    "y = titanic['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#criterion='entropy', max_features=5, n_estimators=100, random_state=0)\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=0) \n",
    "forest.fit(X_train, y_train)\n",
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "print(\"Accuracy of Random Forest: {}\".format(metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_feature_importances(model, features):\n",
    "    n_features = len(features)\n",
    "    plt.barh(np.arange(n_features), model.feature_importances_, align='center')\n",
    "    \n",
    "    plt.yticks(np.arange(n_features), features)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim(-1, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(forest, features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the features with most importance towards predicting survival is Fare, Sex and Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic[['Age', 'Fare', 'Sex']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values.tolist()\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_oh = [dl.one_hot_encode(y, 2) for y in y_train]\n",
    "y_train_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_oh = [dl.one_hot_encode(y, 2) for y in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "    \n",
    "# Name them so we can turn train on and off\n",
    "dropout1 = dl.Dropout(0.1)\n",
    "dropout2 = dl.Dropout(0.1)\n",
    "dropout3 = dl.Dropout(0.1)\n",
    "    \n",
    "t_model = dl.Sequential([\n",
    "    dl.Linear(3, 32),  # Hidden layer 1: size 32\n",
    "    dropout1,\n",
    "    dl.Tanh(),\n",
    "    dl.Linear(32, 16),   # Hidden layer 2: size 16\n",
    "    dropout2,\n",
    "    dl.Tanh(),\n",
    "    dl.Linear(16, 8),   # Hidden layer 3: size 8\n",
    "    dropout3,\n",
    "    dl.Tanh(),\n",
    "    dl.Linear(8, 2)    # Output layer: size 2\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "import tqdm\n",
    "def loop(model: dl.Layer,\n",
    "             images: dl.List[dl.Tensor],\n",
    "             labels: dl.List[dl.Tensor],\n",
    "             loss: dl.Loss,\n",
    "             optimizer: dl.Optimizer = None) -> None:\n",
    "        correct = 0         # Track number of correct predictions.\n",
    "        total_loss = 0.0    # Track total loss.\n",
    "    \n",
    "        with tqdm.trange(len(images)) as t:\n",
    "            for i in t:\n",
    "                predicted = model.forward(images[i])             # Predict.\n",
    "                if argmax(predicted) == argmax(labels[i]):       # Check for\n",
    "                    correct += 1                                 # correctness.\n",
    "                total_loss += loss.loss(predicted, labels[i])    # Compute loss.\n",
    "    \n",
    "                # If we're training, backpropagate gradient and update weights.\n",
    "                if optimizer is not None:\n",
    "                    gradient = loss.gradient(predicted, labels[i])\n",
    "                    model.backward(gradient)\n",
    "                    optimizer.step(model)\n",
    "    \n",
    "                # And update our metrics in the progress bar.\n",
    "                avg_loss = total_loss / (i + 1)\n",
    "                acc = correct / (i + 1)\n",
    "                t.set_description(f\"mnist loss: {avg_loss:.3f} acc: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.deep_learning import main\n",
    "optimizer = dl.Momentum(learning_rate=0.01, momentum=0.99)\n",
    "loss = dl.SoftmaxCrossEntropy()\n",
    "    \n",
    "# Enable dropout and train (takes > 20 minutes on my laptop!)\n",
    "dropout1.train = dropout2.train = dropout3.train = True\n",
    "loop(t_model, X_train, y_train_oh, loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable dropout and evaluate\n",
    "dropout1.train = dropout2.train = dropout3.train = False\n",
    "loop(t_model, X_test, y_test_oh, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [t_model.forward(x) for x in X_test]\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.softmax(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary = [1 if x[1] > 0.5 else 0 for x in dl.softmax(y_pred)]\n",
    "y_pred_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_binary))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, y_pred_binary))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred_binary))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = dl.Momentum(learning_rate=0.001, momentum=0.99)\n",
    "\n",
    "dropout1.train = dropout2.train = dropout3.train = True\n",
    "\n",
    "for _ in range(15):\n",
    "    loop(t_model, X_train, y_train_oh, loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout1.train = dropout2.train = False\n",
    "loop(t_model, X_test, y_test_oh, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [t_model.forward(x) for x in X_test]\n",
    "y_pred_binary = [1 if x[1] > 0.5 else 0 for x in dl.softmax(y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_binary))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, y_pred_binary))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred_binary))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred_binary))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(3)),  # As we have 3 columns in our input data X\n",
    "        layers.Dense(32, activation='tanh'),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(16, activation='tanh'),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(8, activation='tanh'),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(2, activation=\"softmax\")  # Here we specify that we want the last layer to have a softmax activation function\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_keras = np.array(X_train)\n",
    "X_train_keras.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_keras = np.array(y_train_oh)\n",
    "y_train_keras.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.fit(X_train_keras, y_train_keras, batch_size=1, epochs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = keras_model.evaluate(np.array(X_test), np.array(y_test_oh), verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_keras = keras_model.predict(np.array(X_test))\n",
    "y_pred_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_keras_binary = [1 if x[1] > 0.5 else 0 for x in y_pred_keras]\n",
    "y_pred_keras_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_keras_binary))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, y_pred_keras_binary))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred_keras_binary))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred_keras_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. MapReduce (Lecture 13)\n",
    "\n",
    "All exercises from Lecture 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Time Series Analysis (Lecture 14 & 15)\n",
    "\n",
    "Do a time series analysis of the Copenhagen ice cream dataset (\"cph_ice_cream_searches.csv\") from Lectures 14 and 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. IoT (Lecture 17)\n",
    "\n",
    "All exercises from Lecture 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
